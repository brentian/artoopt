<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Chuwen Zhang" />
  <title>QAP</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="assets/pandoc.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
   var mathElements = document.getElementsByClassName("math");
   for (var i = 0; i < mathElements.length; i++) {
    var texText = mathElements[i].firstChild;
    if (mathElements[i].tagName == "SPAN") {
     katex.render(texText.data, mathElements[i], {
      displayMode: mathElements[i].classList.contains('display'),
      throwOnError: false,
      fleqn: false
     });
  }}});
  </script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@8.4.0/dist/mermaid.min.js"></script>
  <script>mermaid.initialize({ startOnLoad: true });</script>
  
</head>
<body>
<header id="title-block-header">
<h1 class="title">QAP</h1>
<p class="author">Chuwen Zhang</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#qap-the-problem">QAP, the problem</a>
<ul>
<li><a href="#differentiation">Differentiation</a></li>
</ul></li>
<li><a href="#mathscr-l_p-regularization"><span class="math inline">\mathscr L_p</span> regularization</a>
<ul>
<li><a href="#mathscr-l_2"><span class="math inline">\mathscr L_2</span></a>
<ul>
<li><a href="#naive">naive</a></li>
</ul></li>
<li><a href="#mathscr-l_2-mathscr-l_1-penalized-formulation"><span class="math inline">\mathscr L_2</span> + <span class="math inline">\mathscr L_1</span> penalized formulation</a>
<ul>
<li><a href="#projected-gradient">Projected gradient</a></li>
<li><a href="#remark">Remark</a></li>
</ul></li>
</ul></li>
<li><a href="#computational-results">Computational Results</a></li>
<li><a href="#reference">Reference</a></li>
</ul>
</nav>
<h1 id="qap-the-problem">QAP, the problem</h1>
<p>QAP, and alternative descriptions, see <span class="citation" data-cites="jiang_l_p-norm_2016"><a href="#ref-jiang_l_p-norm_2016" role="doc-biblioref">1</a></span></p>
<p><span class="math display">\begin{aligned}
&amp;\min_X f(X) = \textrm{tr}(A^\top XB X^\top)  \\
&amp; = \textrm{tr}(X^\top A^\top XB) &amp; x = \textrm{vec}(X)\\
&amp; = \left &lt;\textrm{vec}(X),  \textrm{vec}(A^\top X B )  \right &gt; \\
&amp; = \left &lt;\textrm{vec}(X), B^\top \otimes A^\top \cdot \textrm{vec}(X)  \right &gt; \\ 
&amp; = x^\top (B^\top \otimes A^\top) x\\ 
\mathbf{s.t.} &amp; \\ 
&amp;X \in \Pi_{n}
\end{aligned}</span></p>
<p>is the optimization problem on permutation matrices:</p>
<p><span class="math display"> \Pi_{n}=\left\{X \in \mathbb R ^{n \times n} \mid X e =X^{\top} e = e , X_{i j} \in\{0,1\}\right\}</span></p>
<p>The convex hull of permutation matrices, the Birkhoﬀ polytope, is defined:</p>
<p><span class="math display">D _{n}=\left\{X \in \mathbb R ^{n \times n} \mid X e =X^{\top} e = e , X \geq 0 \right\}</span></p>
<p>for the constraints, also equivalently: <span class="math display">\begin{aligned}
&amp; \textrm{tr}(XX^\top) = \left &lt;x, x \right &gt;_F= n, X \in D_{n}
\end{aligned}</span></p>
<h2 id="differentiation">Differentiation</h2>
<p><span class="math display">\begin{aligned}
&amp;  \nabla f = A^\top XB + AXB^\top \\
&amp; \nabla \textrm{tr}(XX^\top) = 2X
\end{aligned}</span></p>
<h1 id="mathscr-l_p-regularization"><span class="math inline">\mathscr L_p</span> regularization</h1>
<p>various form of regularized problem:</p>
<ul>
<li><p><span class="math inline">\mathscr L_0</span>: <span class="math inline">f(X) + \sigma ||X||_0</span> is exact to the original problem for efficiently large <span class="math inline">\sigma</span> <span class="citation" data-cites="jiang_l_p-norm_2016"><a href="#ref-jiang_l_p-norm_2016" role="doc-biblioref">1</a></span>, but the problem itself is still NP-hard.</p></li>
<li><p><span class="math inline">\mathscr L_p</span>: also suggested by <span class="citation" data-cites="jiang_l_p-norm_2016"><a href="#ref-jiang_l_p-norm_2016" role="doc-biblioref">1</a></span>, good in the sense:</p>
<ul>
<li>strongly concave and the global optimizer must be at vertices</li>
<li><strong>local optimizer is a permutation matrix</strong> if <span class="math inline">\sigma, \epsilon</span> satisfies some condition. Also, there is a lower bound for nonzero entries of the KKT points</li>
</ul></li>
</ul>
<p><span class="math display">\min _{X \in D _{n}} F_{\sigma, p, \epsilon}(X):=f(X)+\sigma\|X+\epsilon 1 \|_{p}^{p}</span></p>
<ul>
<li><span class="math inline">\mathscr L_2</span>, and is based on the fact that <span class="math inline">\Pi_n = D_n \bigcap \{X:\textrm{tr}(XX^\top) = n\}</span>, <span class="citation" data-cites="xia_efficient_2010"><a href="#ref-xia_efficient_2010" role="doc-biblioref">2</a></span></li>
</ul>
<p><span class="math display">\min_Xf(X)+\mu_{0} \cdot \textrm{tr} \left(X X^{\top}\right)</span></p>
<h2 id="mathscr-l_2"><span class="math inline">\mathscr L_2</span></h2>
<h3 id="naive">naive</h3>
<p><span class="math display">\begin{aligned}
&amp;\textrm{tr}(A^\top XB X^\top) + \mu_0 \cdot \textrm{tr}(X X^{\top}) \\
= &amp; x^\top (B^\top \otimes A^\top + \mu\cdot  \mathbf e_{n\times n}) x\\ 
\end{aligned} </span> this implies a LD-like method. (but not exactly)</p>
<h2 id="mathscr-l_2-mathscr-l_1-penalized-formulation"><span class="math inline">\mathscr L_2</span> + <span class="math inline">\mathscr L_1</span> penalized formulation</h2>
<p>Motivated by the formulation using trace:</p>
<p><span class="math display">\begin{aligned}
&amp; \min_X  \textrm{tr}(A^\top XB X^\top) \\
\mathbf{s.t.} &amp;\\
&amp;   \textrm{tr}(XX^\top ) -  n = 0 \\
&amp; X \in D_n
\end{aligned}</span></p>
<p>using absolute value of <span class="math inline">\mathscr L_2</span> penalty and by the factor that <span class="math inline">\forall X \in D_n ,\; \textrm{tr}(XX^\top)\le n</span>, we have:</p>
<p><span class="math display">\begin{aligned}
F_{\mu} &amp; =  f  + \mu\cdot | \textrm{tr}(XX^\top ) -  n| \\
 &amp;= \textrm{tr}(A^\top XB X^\top)  + \mu\cdot n - \mu\cdot \textrm{tr}(XX^\top )
\end{aligned}</span></p>
<p>For sufficiently large penalty parameter <span class="math inline">\mu</span>, the problem solves the original problem.</p>
<p>The penalty method is very likely to become a concave function (even if the original one is convex), and thus it cannot be directly solved by conic solver.</p>
<h3 id="projected-gradient">Projected gradient</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># code see</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>qap_lp.models.qap_model_l2.l2_exact_penalty_gradient_proj</span></code></pre></div>
<p>Suppose we do projection on the penalized problem <span class="math inline">F_\mu</span></p>
<h4 id="derivatives">derivatives</h4>
<p><span class="math display">\begin{aligned}
&amp; \nabla_X F_\mu  = A^\top XB + AXB^\top - 2\mu X \\
&amp; \nabla_\mu F_\mu  = n - \textrm{tr}(XX^\top) \\
&amp; \nabla_\Lambda F_\mu  = - X
\end{aligned}</span></p>
<h4 id="projected-derivative">projected derivative</h4>
<p>problem <em>PD</em>, a quadratic program</p>
<p><span class="math display">\begin{aligned}
&amp;\min_D ||\nabla F_\mu + D ||_F^2  \\
\mathbf{s.t.} &amp; \\
&amp;D e = D^\top e = 0 \\ 
&amp;D_{ij} \ge 0 \quad \textsf{if: } X_{ij} = 0\\
\end{aligned}</span></p>
<ul>
<li>There is no degeneracy, great.</li>
</ul>
<h3 id="remark">Remark</h3>
<h4 id="integrality-of-the-solution">integrality of the solution</h4>
<p>Computational results show that the <em>residue of the trace</em>: <span class="math inline">|n - \textrm{tr}(XX^\top)|</span> is almost zero, this means the algorithm converges to an integral solution. (even without any tuning of penalty parameter <span class="math inline">\mu</span>)</p>
<p>Prove that it is exact if <span class="math inline">\mu</span> is sufficiently large, the model converges to an integral solution.</p>
<p><strong>PF. outline</strong> the model uses exact penalty function, as <span class="math inline">\mu</span>, actually <span class="math inline">\{\mu_k\}</span> become sufficiently large, the penalty method solves the original problem.</p>
<h4 id="quality-of-the-solution">quality of the solution</h4>
<p>it is however hard to find a global optimum, and gradient projection as defined above converges to a local integral solution and then stops, see instances with gap &gt; 10%.</p>
<h4 id="analytic-representation-for-projection">analytic representation for projection</h4>
<p>in projected gradient method, let the space of <span class="math inline">D</span>, (<span class="math inline">e</span> is the vector of 1s)</p>
<p><span class="math display">\mathcal D = \{D\in\mathbb{R}^{n\times n} : \; D e = D^\top e = 0;\; D_{ij} = 0,\;\forall  (i,j) \in M \}</span></p>
<p>is there a way to formulate the set for <span class="math inline">F</span> such that <span class="math inline">\left &lt;F, D \right&gt;_F = 0, \; \forall D\in \mathcal D</span>, can we find an analytic representation?</p>
<h4 id="what-if-projection-is-zero">what if projection is zero?</h4>
<p>dual problem for <span class="math inline">PD</span></p>
<ul>
<li><span class="math inline">\alpha,\beta,\Lambda</span> are Lagrange multipliers, <span class="math inline">\mathbf I</span> is the identity matrix for active constraints of the <span class="math inline">X \ge 0</span> where <span class="math inline">\mathbf I_{ij} = 1</span> if <span class="math inline">X_{ij} = 0</span></li>
</ul>
<p><span class="math display">\begin{aligned}
&amp; L_d = 1/2\cdot ||\nabla F_\mu + D ||_F^2 - \alpha^\top De - \beta^\top D^\top e -\Lambda \circ \mathbf I \bullet D\\
\mathsf{KKT:} &amp; \\
&amp; \nabla F+D - ae^\top - e\beta^\top -\Lambda \circ \mathbf{I} = 0 \\
&amp; \Lambda \ge 0
\end{aligned}</span></p>
<p>Suppose at iteration <span class="math inline">k</span> projected gradient <span class="math inline">D_k = 0</span>, then the KKT condition for</p>
<p>We relax one condition for active inequality for some <span class="math inline">e = (i,j), e \in M</span> such that <span class="math inline">X_e =0</span>, a new optimal direction for problem PD is achieved at <span class="math inline">\hat D</span>, we have:</p>
<p><span class="math display">\begin{aligned}
 &amp; \hat D_{ij} - (\alpha_i + \beta_j) + (\hat \alpha_i + \hat \beta_j) - \Lambda_{ij} = 0, \quad e = (i,j) \\
\end{aligned}</span></p>
<h1 id="computational-results">Computational Results</h1>
<p>The experiments are done on dataset of <a href="http://anjos.mgi.polymtl.ca/qaplib/">QAPLIB</a>, also see paper <span class="citation" data-cites="burkard1997qaplib">[<a href="#ref-burkard1997qaplib" role="doc-biblioref">3</a>]</span></p>
<p>The <span class="math inline">\mathscr L_2</span> penalized <a href="#mathscr-l_2--mathscr-l_1-penalized-formulation">formulation</a> with code <code>qap_lp.models.qap_model_l2.l2_exact_penalty_gradient_proj</code>, solved by gradient projection of module <code>qap_lp.models.qap_gradient_proj</code> can solve almost all instances, except for very large ones (<span class="math inline">\ge</span> 256), it should be better since it now uses Mosek as backend to solve orthogonal projections, line search for step-size, and so on.</p>
<p>current benchmark</p>
<table style="width:86%;">
<caption>L_2 + L_1 penalized gradient projection</caption>
<colgroup>
<col style="width: 12%" />
<col style="width: 23%" />
<col style="width: 15%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>value</th>
<th>rel_gap</th>
<th>trace_res</th>
<th>runtime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>count</td>
<td>115.000</td>
<td>114.000</td>
<td>115.000</td>
<td>115.000</td>
</tr>
<tr class="even">
<td>mean</td>
<td>58087685.641</td>
<td>0.300</td>
<td>2.068</td>
<td>12.905</td>
</tr>
<tr class="odd">
<td>std</td>
<td>201707868.192</td>
<td>0.544</td>
<td>20.853</td>
<td>84.748</td>
</tr>
<tr class="even">
<td>min</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.039</td>
</tr>
<tr class="odd">
<td>25%</td>
<td>3962.001</td>
<td>0.024</td>
<td>0.000</td>
<td>0.125</td>
</tr>
<tr class="even">
<td>50%</td>
<td>97330.031</td>
<td>0.091</td>
<td>0.000</td>
<td>0.215</td>
</tr>
<tr class="odd">
<td>75%</td>
<td>3207963.510</td>
<td>0.255</td>
<td>0.000</td>
<td>1.584</td>
</tr>
<tr class="even">
<td>max</td>
<td>1289655958.000</td>
<td>4.000</td>
<td>223.278</td>
<td>885.492</td>
</tr>
</tbody>
</table>
<h1 class="unnumbered" id="reference">Reference</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-jiang_l_p-norm_2016">
<p>[1] B. Jiang, Y.-F. Liu, and Z. Wen, “L_p-norm regularization algorithms for optimization over permutation matrices,” <em>SIAM Journal on Optimization</em>, vol. 26, no. 4, pp. 2284–2313, 2016.</p>
</div>
<div id="ref-xia_efficient_2010">
<p>[2] Y. Xia, “An efficient continuation method for quadratic assignment problems,” <em>Computers &amp; Operations Research</em>, vol. 37, no. 6, pp. 1027–1032, 2010.</p>
</div>
<div id="ref-burkard1997qaplib">
<p>[3] R. E. Burkard, S. E. Karisch, and F. Rendl, “QAPLIB–a quadratic assignment problem library,” <em>Journal of Global optimization</em>, vol. 10, no. 4, pp. 391–403, 1997.</p>
</div>
</div>
</body>
</html>
