<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Chuwen Zhang" />
  <title>QAP</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="assets/pandoc.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
   var mathElements = document.getElementsByClassName("math");
   var macros = [];
   for (var i = 0; i < mathElements.length; i++) {
    var texText = mathElements[i].firstChild;
    if (mathElements[i].tagName == "SPAN") {
     katex.render(texText.data, mathElements[i], {
      displayMode: mathElements[i].classList.contains('display'),
      throwOnError: false,
      macros: macros,
      fleqn: false
     });
  }}});
  </script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@8.4.0/dist/mermaid.min.js"></script>
  <script>mermaid.initialize({ startOnLoad: true });</script>
  
</head>
<body>
<header id="title-block-header">
<h1 class="title">QAP</h1>
<p class="author">Chuwen Zhang</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#the-problem">The Problem</a>
<ul>
<li><a href="#differentiation">Differentiation</a></li>
</ul></li>
<li><a href="#mathscr-l_p-regularization"><span class="math inline">\mathscr L_p</span> regularization</a>
<ul>
<li><a href="#mathscr-l_2"><span class="math inline">\mathscr L_2</span></a>
<ul>
<li><a href="#naive">naive</a></li>
<li><a href="#a-better-naive">a better naive</a></li>
</ul></li>
<li><a href="#mathscr-l_2-mathscr-l_1-penalized-formulation"><span class="math inline">\mathscr L_2</span> + <span class="math inline">\mathscr L_1</span> penalized formulation</a>
<ul>
<li><a href="#rosens-projected-gradient">Rosen’s Projected Gradient</a></li>
<li><a href="#goldstein-levitin-poljak-projected-gradient">Goldstein-Levitin-Poljak Projected Gradient</a></li>
</ul></li>
</ul></li>
<li><a href="#reference">Reference</a></li>
</ul>
</nav>
<h1 id="the-problem">The Problem</h1>
<p>QAP, and alternative descriptions, see <span class="citation" data-cites="jiang_l_p-norm_2016"><a href="#ref-jiang_l_p-norm_2016" role="doc-biblioref">Jiang et al.</a> (<a href="#ref-jiang_l_p-norm_2016" role="doc-biblioref">2016</a>)</span></p>
<p><span class="math display">\begin{aligned}
\min_X f(X) &amp;= \textrm{tr}(A^\top XB X^\top)  \\
&amp; = \textrm{tr}(X^\top A^\top XB) &amp; x = \textrm{vec}(X)\\
&amp; = x^\top (B^\top \otimes A^\top) x\\ 
\mathbf{s.t.} &amp; \\ 
&amp;X \in \Pi_{n}
\end{aligned}</span></p>
<p>is the optimization problem on permutation matrices:</p>
<p><span class="math display"> \Pi_{n}=\left\{X \in \mathbb R ^{n \times n} \mid X e =X^{\top} e = e , X_{i j} \in\{0,1\}\right\}</span></p>
<p>The convex hull of permutation matrices, the Birkhoﬀ polytope, is defined:</p>
<p><span class="math display">D _{n}=\left\{X \in \mathbb R ^{n \times n} \mid X e =X^{\top} e = e , X \geq 0 \right\}</span></p>
<p>for the integral constraints, also equivalently: <span class="math display">\begin{aligned}
&amp; \textrm{tr}(XX^\top) = \left &lt;x, x \right &gt;_F= n, X \in D_{n}
\end{aligned}</span></p>
<h2 id="differentiation">Differentiation</h2>
<p><span class="math display">\begin{aligned}
&amp;  \nabla f = A^\top XB + AXB^\top \\
&amp; \nabla \textrm{tr}(XX^\top) = 2X
\end{aligned}</span></p>
<h1 id="mathscr-l_p-regularization"><span class="math inline">\mathscr L_p</span> regularization</h1>
<p>various form of regularized problem:</p>
<ul>
<li><span class="math inline">\mathscr L_0</span>, <span class="math inline">f(X) + \sigma ||X||_0</span> is exact to the original problem for efficiently large <span class="math inline">\sigma</span> <span class="citation" data-cites="jiang_l_p-norm_2016"><a href="#ref-jiang_l_p-norm_2016" role="doc-biblioref">Jiang et al.</a> (<a href="#ref-jiang_l_p-norm_2016" role="doc-biblioref">2016</a>)</span>, but the problem itself is still NP-hard.</li>
</ul>
<p><span class="math display">\min _{X \in D _{n}} F_{\sigma, p, \epsilon}(X):=f(X)+\sigma\|X+\epsilon 1 \|_{p}^{p}</span></p>
<ul>
<li><span class="math inline">\mathscr L_2</span>, and is based on the fact that <span class="math inline">\Pi_n = D_n \bigcap \{X:\textrm{tr}(XX^\top) = n\}</span>, <span class="citation" data-cites="xia_efficient_2010"><a href="#ref-xia_efficient_2010" role="doc-biblioref">Xia</a> (<a href="#ref-xia_efficient_2010" role="doc-biblioref">2010</a>)</span>, see <a href="#mathscr-l_2">implementations</a></li>
</ul>
<p><span class="math display">\min_Xf(X)+\mu_{0} \cdot \textrm{tr} \left(X X^{\top}\right)</span></p>
<ul>
<li><span class="math inline">\mathscr L_2</span>, using penalized objective, see <a href="#mathscr-l_2--mathscr-l_1-penalized-formulation">implementations</a></li>
</ul>
<p><span class="math display">\textrm{tr}(A^\top XB X^\top)  + \mu\cdot n - \mu\cdot \textrm{tr}(XX^\top )</span></p>
<ul>
<li><span class="math inline">\mathscr L_p, 0&lt;p&lt; 1</span>, also suggested by <span class="citation" data-cites="jiang_l_p-norm_2016"><a href="#ref-jiang_l_p-norm_2016" role="doc-biblioref">Jiang et al.</a> (<a href="#ref-jiang_l_p-norm_2016" role="doc-biblioref">2016</a>)</span>, good in the sense:
<ul>
<li>strongly concave and the global optimizer must be at vertices</li>
<li><strong>local optimizer is a permutation matrix</strong> if <span class="math inline">\sigma, \epsilon</span> satisfies some condition. Also, there is a lower bound for nonzero entries of the KKT points</li>
<li>reproduced results</li>
</ul></li>
</ul>
<h2 id="mathscr-l_2"><span class="math inline">\mathscr L_2</span></h2>
<h3 id="naive">naive</h3>
<p><span class="math display">\begin{aligned}
&amp;\min_X\textrm{tr}(A^\top XB X^\top) + \mu_0 \cdot \textrm{tr}(X X^{\top}) \\
= &amp; x^\top (B^\top \otimes A^\top + \mu\cdot  \mathbf e_{n\times n}) x\\ 
\end{aligned} </span> this implies a LD-like method. (but not exactly)</p>
<h3 id="a-better-naive">a better naive</h3>
<p><span class="math display">\begin{aligned}
&amp;\min_X \mathbf{tr}(M^\top SM) \\&amp;M=\begin{pmatrix}XB\\AX\end{pmatrix},\; S = \begin{pmatrix}\bf{0} &amp; \frac{1}{2}\mathbf{I}\\\frac{1}{2}\mathbf{I} &amp; \bf{0} \end{pmatrix} 
\end{aligned}</span> factorizing matrix <span class="math inline">S</span> by scale factor <span class="math inline">\delta</span> <span class="math display">R^\top R = S + \delta I</span></p>
<h2 id="mathscr-l_2-mathscr-l_1-penalized-formulation"><span class="math inline">\mathscr L_2</span> + <span class="math inline">\mathscr L_1</span> penalized formulation</h2>
<p>Motivated by the formulation using trace:</p>
<p><span class="math display">\begin{aligned}
&amp; \min_X  \textrm{tr}(A^\top XB X^\top) \\
\mathbf{s.t.} &amp;\\
&amp;   \textrm{tr}(XX^\top ) -  n = 0 \\
&amp; X \in D_n
\end{aligned}</span></p>
<p>using absolute value of <span class="math inline">\mathscr L_2</span> penalty and by the factor that <span class="math inline">\forall X \in D_n ,\; \textrm{tr}(XX^\top)\le n</span>, we have:</p>
<p><span class="math display">\begin{aligned}
F_{\mu} &amp; =  f  + \mu\cdot | \textrm{tr}(XX^\top ) -  n| \\
 &amp;= \textrm{tr}(A^\top XB X^\top)  + \mu\cdot n - \mu\cdot \textrm{tr}(XX^\top )
\end{aligned}</span></p>
<p>For sufficiently large penalty parameter <span class="math inline">\mu</span>, the problem solves the original problem.</p>
<p>The penalty method is very likely to become a concave function (even if the original one is convex), and thus it cannot be directly solved by conic solver.</p>
<h3 id="rosens-projected-gradient">Rosen’s Projected Gradient</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># code see</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>qap.models.qap_model_l2.l2_exact_penalty_gradient_proj</span></code></pre></div>
<p>Suppose we do projection on the penalized problem <span class="math inline">F_\mu</span></p>
<h4 id="derivatives">derivatives</h4>
<p><span class="math display">\begin{aligned}
&amp; \nabla_X F_\mu  = A^\top XB + AXB^\top - 2\mu X \\
&amp; \nabla_\mu F_\mu  = n - \textrm{tr}(XX^\top) \\
&amp; \nabla_\Lambda F_\mu  = - X
\end{aligned}</span></p>
<h4 id="projected-derivative">projected derivative</h4>
<p>problem <em>PD</em>, a quadratic program</p>
<p><span class="math display">\begin{aligned}
&amp;\min_D ||\nabla F_\mu + D ||_F^2  \\
\mathbf{s.t.} &amp; \\
&amp;D e = D^\top e = 0 \\ 
&amp;D_{ij} \ge 0 \quad \textsf{if: } X_{ij} = 0\\
\end{aligned}</span></p>
<p>or equivalently, a linear program (must add norm constraints to avoid unbounded objective)</p>
<p><span class="math display">\begin{aligned}
&amp;\min_D \nabla F_\mu \bullet D   \\
\mathbf{s.t.} &amp; \\
&amp;D e = D^\top e = 0 \\ 
&amp;D_{ij} \ge 0 \quad \textsf{if: } X_{ij} = 0\\ 
&amp;||D||\le 1 \\
\end{aligned}</span></p>
<ul>
<li>There is no degeneracy, great.</li>
</ul>
<h4 id="remark">Remark</h4>
<blockquote>
<p>integrality of the solution</p>
</blockquote>
<p>Computational results show that the <em>residue of the trace</em>: <span class="math inline">|n - \textrm{tr}(XX^\top)|</span> is almost zero, this means the algorithm converges to an integral solution. (even without any tuning of penalty parameter <span class="math inline">\mu</span>)</p>
<p>Prove that it is exact if <span class="math inline">\mu</span> is sufficiently large, the model converges to an integral solution.</p>
<p><strong>PF. outline</strong> <strong>a</strong>. the model uses exact penalty function, as <span class="math inline">\mu</span>, actually <span class="math inline">\{\mu_k\}</span> become sufficiently large, the penalty method solves the original problem. <strong>b</strong>. if <span class="math inline">\{\mu_k\}</span> become sufficiently large, the penalized objective will be concave, so that the optimal solution should be attained at the vertices.</p>
<blockquote>
<p>quality of the solution</p>
</blockquote>
<p>it is however hard to find a global optimum, and gradient projection as defined above converges to a local integral solution and then stops, see instances with gap &gt; 10%.</p>
<blockquote>
<p>analytic representation for projection</p>
</blockquote>
<p>in projected gradient method, let the space of <span class="math inline">D</span>, (<span class="math inline">e</span> is the vector of 1s)</p>
<p><span class="math display">\mathcal D = \{D\in\mathbb{R}^{n\times n} : \; D e = D^\top e = 0;\; D_{ij} = 0,\;\forall  (i,j) \in M \}</span></p>
<p>is there a way to formulate the set for <span class="math inline">F</span> such that <span class="math inline">\left &lt;F, D \right&gt;_F = 0, \; \forall D\in \mathcal D</span>, can we find an analytic representation?</p>
<blockquote>
<p>*what if projection is zero?</p>
</blockquote>
<p>dual problem for <span class="math inline">PD</span></p>
<ul>
<li><span class="math inline">\alpha,\beta,\Lambda</span> are Lagrange multipliers, <span class="math inline">\mathbf I</span> is the identity matrix for active constraints of the <span class="math inline">X \ge 0</span> where <span class="math inline">\mathbf I_{ij} = 1</span> if <span class="math inline">X_{ij} = 0</span></li>
</ul>
<p><span class="math display">\begin{aligned}
&amp; L_d = 1/2\cdot ||\nabla F_\mu + D ||_F^2 - \alpha^\top De - \beta^\top D^\top e -\Lambda \circ \mathbf I \bullet D\\
\mathsf{KKT:} &amp; \\
&amp; \nabla F+D - ae^\top - e\beta^\top -\Lambda \circ \mathbf{I} = 0 \\
&amp; \Lambda \ge 0
\end{aligned}</span></p>
<p>Suppose at iteration <span class="math inline">k</span> projected gradient <span class="math inline">D_k = 0</span>, then the KKT condition for</p>
<p>We relax one condition for active inequality for some <span class="math inline">e = (i,j), e \in M</span> such that <span class="math inline">X_e =0</span>, a new optimal direction for problem PD is achieved at <span class="math inline">\hat D</span>, we have:</p>
<p><span class="math display">\begin{aligned}
 &amp; \hat D_{ij} - (\alpha_i + \beta_j) + (\hat \alpha_i + \hat \beta_j) - \Lambda_{ij} = 0, \quad e = (i,j) \\
\end{aligned}</span></p>
<p>You should exchange the most negative <span class="math inline">\Lambda_{ij}</span></p>
<h3 id="goldstein-levitin-poljak-projected-gradient">Goldstein-Levitin-Poljak Projected Gradient</h3>
<p>This is a better known projected gradient method. ## # Computational Results</p>
<p>The experiments are done on dataset of <a href="http://anjos.mgi.polymtl.ca/qaplib/">QAPLIB</a>, also see paper <span class="citation" data-cites="burkard1997qaplib">(<a href="#ref-burkard1997qaplib" role="doc-biblioref">Burkard et al. 1997</a>)</span></p>
<p>The <span class="math inline">\mathscr L_2</span> penalized <a href="#mathscr-l_2--mathscr-l_1-penalized-formulation">formulation</a> with code <code>qap.models.qap_model_l2.l2_exact_penalty_gradient_proj</code>, solved by gradient projection of module <code>qap.models.qap_gradient_proj</code> can solve almost all instances, except for very large ones (<span class="math inline">\ge</span> 256), it should be better since it now uses Mosek as backend to solve orthogonal projections, line search for step-size, and so on.</p>
<p>current benchmark</p>
<pre class="table" data-caption="L_2 + L_1 penalized gradient projection" data-source="l2_grad_proj_benchmark.20201012.csv"><code></code></pre>
<h1 class="unnumbered" id="reference">Reference</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-burkard1997qaplib" class="csl-entry" role="doc-biblioentry">
Burkard RE, Karisch SE, Rendl F (1997) QAPLIB–a quadratic assignment problem library. <em>Journal of Global optimization</em> 10(4):391–403.
</div>
<div id="ref-jiang_l_p-norm_2016" class="csl-entry" role="doc-biblioentry">
Jiang B, Liu YF, Wen Z (2016) L_p-norm regularization algorithms for optimization over permutation matrices. <em><span>SIAM</span> Journal on Optimization</em> 26(4):2284–2313.
</div>
<div id="ref-xia_efficient_2010" class="csl-entry" role="doc-biblioentry">
Xia Y (2010) An efficient continuation method for quadratic assignment problems. <em>Computers &amp; Operations Research</em> 37(6):1027–1032.
</div>
</div>
</body>
</html>
